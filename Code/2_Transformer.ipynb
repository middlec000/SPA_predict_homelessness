{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b4debe-3945-44f6-b897-ce130a1761a2",
   "metadata": {},
   "source": [
    "# Resources\n",
    "https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3\n",
    "\n",
    "https://github.com/ntakouris/timeseries-pretrain-tests/blob/main/transformers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2ce1a-9362-4e51-88e2-2e3e439077a8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6675a933-6e51-4a9b-9495-72570f39332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from custom_methods import model_eval\n",
    "\n",
    "datapath = '../Data/'\n",
    "\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6670f-c8d2-4c89-adfb-8149add3e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ann_ready.pickle'\n",
    "infile = open(datapath+filename,'rb')\n",
    "data = pickle.load(infile)\n",
    "X = data['X']\n",
    "Y = data['Y']\n",
    "infile.close()\n",
    "\n",
    "print(type(X))\n",
    "print('Format: [num_data_objects, max_sequence_length, num_vars]')\n",
    "print('\\nX')\n",
    "print(X[1])\n",
    "print('\\nY')\n",
    "print(Y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e916240-0ac0-4cf3-9778-32bc119a51e1",
   "metadata": {},
   "source": [
    "## Split Training, Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e226c14-f5c7-4ec0-8911-5d897b981c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "for i in range(X.shape[1]):\n",
    "    scalers[i] = StandardScaler()\n",
    "    X[:, i, :] = scalers[i].fit_transform(X[:, i, :])\n",
    "\n",
    "TEST_FRAC = 1/3\n",
    "\n",
    "# Split Train/Test sets\n",
    "df_train, df_test = model_eval.split_on_people(df, id_col=id_col, test_frac=TEST_FRAC)\n",
    "\n",
    "# Split X and Y\n",
    "X_train = df_train.drop('CMIS_MATCH', axis=1)\n",
    "Y_train = df_train[['CMIS_MATCH', id_col]].groupby(id_col).first()\n",
    "\n",
    "X_test = df_test.drop('CMIS_MATCH', axis=1)\n",
    "Y_test = df_test[['CMIS_MATCH', id_col]].groupby(id_col).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6729e751-53d6-489e-90d5-c7a9fda5e932",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e45c7-243b-4937-9625-f98599d36ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom time embedding layer\n",
    "# https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3\n",
    "\n",
    "class Time2Vec(keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=1):\n",
    "        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n",
    "        self.k = kernel_size\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # trend\n",
    "        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
    "        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
    "        # periodic\n",
    "        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
    "        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
    "        super(Time2Vec, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        bias = self.wb * inputs + self.bb\n",
    "        dp = K.dot(inputs, self.wa) + self.ba\n",
    "        wgts = K.sin(dp) # or K.cos(.)\n",
    "\n",
    "        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n",
    "        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n",
    "        return ret\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]*(self.k + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52fc5d-9f9e-4520-ace9-c6cd61235c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention block\n",
    "# https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3\n",
    "\n",
    "from tensorflow_addons.layers import MultiHeadAttention\n",
    "\n",
    "class AttentionBlock(keras.Model):\n",
    "    def __init__(self, name='AttentionBlock', num_heads=2, head_size=128, ff_dim=None, dropout=0, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "        if ff_dim is None:\n",
    "            ff_dim = head_size\n",
    "\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, head_size=head_size, dropout=dropout)\n",
    "        self.attention_dropout = keras.layers.Dropout(dropout)\n",
    "        self.attention_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.ff_conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n",
    "        # self.ff_conv2 at build()\n",
    "        self.ff_dropout = keras.layers.Dropout(dropout)\n",
    "        self.ff_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.ff_conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1) \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.attention([inputs, inputs])\n",
    "        x = self.attention_dropout(x)\n",
    "        x = self.attention_norm(inputs + x)\n",
    "\n",
    "        x = self.ff_conv1(x)\n",
    "        x = self.ff_conv2(x)\n",
    "        x = self.ff_dropout(x)\n",
    "\n",
    "        x = self.ff_norm(inputs + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c41b825-ed2c-43de-b229-ad0fe0086dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3\n",
    "\n",
    "class ModelTrunk(keras.Model):\n",
    "      def __init__(self, name='ModelTrunk', time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n",
    "        if ff_dim is None:\n",
    "            ff_dim = head_size\n",
    "        self.dropout = dropout\n",
    "        self.attention_layers = [AttentionBlock(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range(num_layers)]\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        time_embedding = keras.layers.TimeDistributed(self.time2vec)(inputs)\n",
    "        x = K.concatenate([inputs, time_embedding], -1)\n",
    "        for attention_layer in self.attention_layers:\n",
    "            x = attention_layer(x)\n",
    "\n",
    "        return K.reshape(x, (-1, x.shape[1] * x.shape[2])) # flat vector of features out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db2ac1-b754-4242-9297-0f28ae833e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom learning rate\n",
    "# https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3\n",
    "\n",
    "def lr_scheduler(epoch, lr, warmup_epochs=15, decay_epochs=100, initial_lr=1e-6, base_lr=1e-3, min_lr=5e-5):\n",
    "    if epoch <= warmup_epochs:\n",
    "        pct = epoch / warmup_epochs\n",
    "        return ((base_lr - initial_lr) * pct) + initial_lr\n",
    "\n",
    "    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n",
    "        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n",
    "        return ((base_lr - min_lr) * pct) + min_lr\n",
    "\n",
    "    return min_lr\n",
    "\n",
    "callbacks += [keras.callbacks.LearningRateScheduler(partial(lr_scheduler, ...), verbose=0)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
